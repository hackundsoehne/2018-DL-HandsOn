{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from math import exp\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random stuff:\n",
    " - https://playground.tensorflow.org/\n",
    " - https://fosterelli.co/executing-gradient-descent-on-the-earth\n",
    " - https://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf\n",
    " \n",
    " # ROUGH DRAFT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We've got the forward propagation, but how to train the neural network?\n",
    " - many approaches, but Backward-Propagation is enabling the whole Deep-Learning Hype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for BP to work, **everything** needs to be differentiable!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition: Learning as an optimization problem\n",
    "in theory, it's easy!\n",
    " - assign every instance of neural network a score through a differentiable function (error function), e.g. derivation from target\n",
    " - optimize the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](/assets/Extrema_example.svg)\n",
    "\n",
    "By I, KSmrq, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2276449"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://www.cs.umd.edu/~tomg/img/landscapes/noshort.png)\n",
    "[source](https://arxiv.org/abs/1712.09913)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BP-Rule\n",
    "nothing more than the chain rule!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](/assets/mlp_model.svg)\n",
    "\n",
    "TODO real layer formalization\n",
    "\n",
    "$E$ is the error function, $w_{ijz}$ is some weight $z$ of neuron $j$ in layer $z$, $x_i$ is the result of layer $i$, $L_{ij}$ is the $i$-th layer for Neuron $j$ ($L_{ij}(x) = \\sum_{x \\in i} n_x(x)$), $n_ij = $\n",
    " - Gradient of our Error Function: $\\triangledown E = \\{\\frac{\\partial E}{\\partial w_{111}}, \\frac{\\partial E}{\\partial w_{112}}, ..., \\frac{\\partial E}{\\partial w_{l_1l_2l_3}}\\}$\n",
    " - update weight: $\\Delta w_{ij} = - \\lambda * \\frac{\\partial E}{\\partial w_i}$, where $\\lambda$ is the learning rate (\"step size\")\n",
    " \n",
    "#### calculating the Delta for each weight indipendently is super expensive!!\n",
    " - Delta rule to the rescue! $[f \\circ g (x))]'=f'(g(x))*g'(x)$ + layering is function composition\n",
    " - backpropagation in a nutshell: \n",
    " $\\Delta w_{ij} = - \\lambda * (\\frac{\\partial L_{i+1}}{\\partial n_i(x_{i-1}))}*\\frac{\\partial n_i(x_{i-1})}{\\partial w_{ij}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP on Loss-Function\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP though Activation Functions\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP on weights and biases\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
